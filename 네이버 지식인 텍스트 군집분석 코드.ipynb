{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966f37c6",
   "metadata": {},
   "source": [
    "# 1. 데이터 수집 - 기간 설정해서 네이버 지식인 크롤링 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f892e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지\n",
    "from selenium import webdriver  \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request as urlreq\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "from openpyxl import load_workbook \n",
    "import openpyxl as xl\n",
    "\n",
    "\n",
    "# 키워드, 기간설정, 페이지 번호에 따른 url 가져오는 함수 생성\n",
    "def mk_url(keyword, from_date, to_date, page_num):\n",
    "    \n",
    "    #최신순으로 정렬한 path\n",
    "    path = 'https://kin.naver.com/search/list.naver?sort=date&query='\n",
    "    \n",
    "    period = '&period=' + from_date + '.%7C' + to_date\n",
    "    url = path + keyword + period + '.&section=qna&page=' + page_num\n",
    "    \n",
    "    return url\n",
    "\n",
    "# 기간설정을 위한 날짜 만드는 함수 생성\n",
    "def mk_date(month, day):\n",
    "    f = '2022.%s.%s'%(month.zfill(2), day.zfill(2))\n",
    "    t = '2022.%s.%s'%(month.zfill(2), day.zfill(2))\n",
    "    return f, t\n",
    "\n",
    "\n",
    "############키워드, 날짜 설정하기 위한 변수 설정 부분############\n",
    "# 한달마다 기간설정하여 크롤링 진행\n",
    "keyword = '우울증'\n",
    "month = 3\n",
    "s_day = 1\n",
    "e_day = 31\n",
    "################################################################\n",
    "\n",
    "\n",
    "date_info = str(month).zfill(2) + str(s_day).zfill(2) + '_' + str(month).zfill(2) + str(e_day).zfill(2) \n",
    "f_name = keyword + '_' + date_info +\"_crawling_result.xlsx\"\n",
    "\n",
    "#엑셀로 저장하기 위한 변수 설정\n",
    "wb = xl.Workbook()\n",
    "sheet = wb.active\n",
    "sheet.append(['date','user_name','title','content','tag'])\n",
    "\n",
    "\n",
    "# 크롤링 수행\n",
    "for i in tqdm_notebook(range(s_day, e_day + 1), desc = '페이지 수집중...'):\n",
    "    f, t = mk_date(str(month), str(i))\n",
    "    post_number = None\n",
    "    current_number = None\n",
    "    total_number = None\n",
    "    page_num = 1\n",
    "    page_url = []\n",
    "    \n",
    "    # 1차 크롤링 - 페이지별로 게시글 url 수집\n",
    "    while True:\n",
    "        #최신순으로 크롤링\n",
    "        \n",
    "        # 크롬 드라이버\n",
    "        binary = \"C:\\\\data\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "\n",
    "        # 브라우저를 인스턴스화\n",
    "        driver = webdriver.Chrome(binary)\n",
    "\n",
    "        kwd_url = mk_url(keyword,f,t,str(page_num))\n",
    "        driver.get(kwd_url)\n",
    "        time.sleep(0.5)\n",
    "        html = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        base = soup.find_all('a',class_=\"_nclicks:qna.txt _searchListTitleAnchor\")\n",
    "\n",
    "        # 개별 주소 가져오기\n",
    "        for b_num in base:\n",
    "            page_url.append(b_num.get('href')) \n",
    "\n",
    "        # 페이지 넘기기\n",
    "        post_number = driver.find_element_by_class_name('number').text\n",
    "        \n",
    "        current_number = post_number[post_number.find('-')+1:post_number.find('/')]\n",
    "        current_number = re.sub(',','', current_number)\n",
    "        \n",
    "        total_number = post_number[post_number.find('/')+1:post_number.find(')')]\n",
    "        total_number = re.sub(',','', total_number)\n",
    "        \n",
    "        if int(current_number) == int(total_number):\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            page_num += 1\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "    page_url = list(set(page_url))  # 페이지 url 중복제거\n",
    "\n",
    "    # 2차 크롤링 - 각 페이지 게시내용 크롤링\n",
    "    for i in tqdm_notebook(page_url, desc = '게시글 수집중....'):\n",
    "        try:\n",
    "            driver.get(i)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            #upload_date\n",
    "            upload_date = driver.find_element_by_class_name(\"c-userinfo__info\").text\n",
    "            upload_date = upload_date[upload_date.find('\\n')+1:]\n",
    "\n",
    "            #user name\n",
    "            user_name = driver.find_element_by_class_name(\"c-userinfo__author\").text\n",
    "            user_name = user_name[user_name.find('\\n')+1:]\n",
    "\n",
    "            #제목\n",
    "            title = driver.find_element_by_class_name('title').text\n",
    "\n",
    "            #내용\n",
    "            try:\n",
    "                question_txt = driver.find_element_by_class_name('c-heading__content').text\n",
    "            except:\n",
    "                question_txt = \"\"\n",
    "\n",
    "            question_txt = re.sub('[\\n\\r\\t]+','',question_txt)\n",
    "\n",
    "            #태그\n",
    "            tag = driver.find_element_by_class_name(\"tag-list__item.tag-list__item--category\").text\n",
    "            tag = tag[tag.find('Ξ')+2:]\n",
    "\n",
    "            # 엑셀에 게시날짜, 제목, 내용, 태그 저장\n",
    "            sheet.append([upload_date, user_name, title, question_txt, tag])\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "wb.save(\"c:\\\\naver_qna\\\\\" + f_name)  # 엑셀로 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822d470",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리 \n",
    "1) 크롤링된 데이터에서 수집기간에 맞지 않은 데이터들을 제외하고 하나의 데이터 프레임으로 병합하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "doc1 = pd.read_excel(\"c:\\\\naver_qna\\\\우울증_0101_0115_crawling_result.xlsx\")\n",
    "doc2 = pd.read_excel(\"c:\\\\naver_qna\\\\우울증_0116_0131_crawling_result.xlsx\")\n",
    "doc3 = pd.read_excel(\"c:\\\\naver_qna\\\\우울증_0201_0228_crawling_result.xlsx\")\n",
    "doc4 = pd.read_excel(\"c:\\\\naver_qna\\\\우울증_0301_0331_crawling_result.xlsx\")\n",
    "doc5 = pd.read_excel(\"c:\\\\naver_qna\\\\우울증_0401_0415_crawling_result.xlsx\")\n",
    "doc6 = pd.read_excel(\"c:\\\\naver_qna\\\\우울증_0416_0430_crawling_result.xlsx\")\n",
    "\n",
    "\n",
    "doc1.date = pd.to_datetime(doc1.date)\n",
    "doc2.date = pd.to_datetime(doc2.date)\n",
    "doc3.date = pd.to_datetime(doc3.date)\n",
    "doc4.date = pd.to_datetime(doc4.date)\n",
    "doc5.date = pd.to_datetime(doc5.date)\n",
    "doc6.date = pd.to_datetime(doc6.date)\n",
    "\n",
    "\n",
    "doc1 = doc1[(doc1.date >= '2022-01-01')&(doc1.date <= '2022-01-15')]\n",
    "doc2 = doc2[(doc2.date >= '2022-01-16')&(doc2.date <= '2022-01-31')]\n",
    "doc3 = doc3[(doc3.date >= '2022-02-01')&(doc3.date <= '2022-02-28')]\n",
    "doc4 = doc4[(doc4.date >= '2022-03-01')&(doc4.date <= '2022-03-31')]\n",
    "doc5 = doc5[(doc5.date >= '2022-04-01')&(doc5.date <= '2022-04-15')]\n",
    "doc6 = doc6[(doc6.date >= '2022-04-16')&(doc6.date <= '2022-04-30')]\n",
    "\n",
    "\n",
    "result = pd.concat([doc1,doc2,doc3,doc4,doc5,doc6]).reset_index()\n",
    "result = result.iloc[:,1:]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc0930",
   "metadata": {},
   "source": [
    "2) 우울, 불안, 부정과 관련없는 데이터를 제거하기 위해 임의로 우울 기준 단어를 설정.  \n",
    "기준에 맞지 않는 글을을 제거하였음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d677a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = result.drop_duplicates()\n",
    "result2\n",
    "\n",
    "word = ['우울','불안','슬퍼','힘든','힘들','죽고','살고','살아야','스트레스','무기력','자살','ㅈㅏㅅㅏㄹ','공황','병원',\n",
    "       '위로','분노','자존감','조울','상담','치료','눈물','외로','감정','살려','걱정','아파요','불면증','싫어요','죽어도',\n",
    "       '트라우마','상처', '방황','중독','싫어','울고', '죽으면','폭력', '극단적']\n",
    "\n",
    "ex_word = result2\n",
    "\n",
    "for i in word:\n",
    "    ex_word = ex_word[:][ex_word.content.apply(lambda x: str(x).count(i) == 0 ) ]\n",
    "    ex_word = ex_word[:][ex_word.title.apply(lambda x: str(x).count(i) == 0 ) ]\n",
    "\n",
    "ex_word\n",
    "\n",
    "data_raw = result2.drop(ex_word.index)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67edbf43",
   "metadata": {},
   "source": [
    "3) 문자열 정제  \n",
    "한글과 공백이 아닌 문자열은 모두 제거하였음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw\n",
    "data.content = data.content.str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "data.title = data.title.str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207dd906",
   "metadata": {},
   "source": [
    "4) 토큰화 \n",
    "Konlpy의 okt로 텍스트를 토큰화하였음.\n",
    "- 명사만 추출\n",
    "- 품사태깅하여 명사, 형용사, 동사만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db342dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사만 추출\n",
    "from tqdm import tqdm\n",
    "\n",
    "noun = []\n",
    "data.content = data.content.astype('str')\n",
    "for sentence in tqdm(data.content):\n",
    "    sentence = okt.normalize(sentence)\n",
    "    noun.append(okt.nouns(sentence))  # 형태소 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "noun_list = []\n",
    "for i in noun:\n",
    "    for k in i:\n",
    "        if len(k) > 1:\n",
    "            noun_list.append(k)\n",
    "noun_list\n",
    "\n",
    "c = Counter(noun_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작성내용에서 텍스트 명사로만 추출하여 워드클라우드 생성\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open('c:\\\\naver_qna\\\\head.png')\n",
    "img_array = np.array(img)\n",
    "\n",
    "wc = WordCloud(background_color ='white',colormap='gist_earth',\n",
    "               width = 700, height = 700, font_path='H2GTRE',\n",
    "               max_font_size=300, max_words=150, mask = img_array)\n",
    "\n",
    "gen = wc.generate_from_frequencies(c)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.rc('font',family = 'Malgun Gothic')  # 한글 안깨지게 하는 코드\n",
    "plt.imshow(gen)\n",
    "plt.title('[작성내용 단어 빈도에 따른 워드클라우드]\\n', size = 20)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "wc.to_file('c:\\\\naver_qna\\\\qna_wordcloud_con.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d38e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작성제목에서 텍스트 명사로만 추출하여 워드클라우드 생성\n",
    "from tqdm import tqdm\n",
    "\n",
    "noun = []\n",
    "data.title = data.title.astype('str')\n",
    "for sentence in tqdm(data.title):\n",
    "    sentence = okt.normalize(sentence)\n",
    "    noun.append(okt.nouns(sentence))  # 형태소 추출\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "noun_list = []\n",
    "for i in noun:\n",
    "    for k in i:\n",
    "        if len(k) > 1:\n",
    "            noun_list.append(k)\n",
    "noun_list\n",
    "\n",
    "c = Counter(noun_list)\n",
    "df = pd.DataFrame(c.most_common(10), columns = ['word', 'count'])\n",
    "df.index = df.index + 1\n",
    "df\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open('c:\\\\naver_qna\\\\head.png')\n",
    "img_array = np.array(img)\n",
    "\n",
    "wc = WordCloud(background_color ='white',colormap='gist_heat',\n",
    "               width = 700, height = 700, font_path='H2GTRE',\n",
    "               max_font_size=300, max_words=150, mask = img_array)\n",
    "\n",
    "gen = wc.generate_from_frequencies(c)\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.rc('font',family = 'Malgun Gothic')  # 한글 안깨지게 하는 코드\n",
    "plt.imshow(gen)\n",
    "plt.title('[작성제목 단어 빈도에 따른 워드클라우드]\\n', size = 20)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "wc.to_file('c:\\\\naver_qna\\\\qna_wordcloud_tit.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41691bad",
   "metadata": {},
   "source": [
    "# 3. word2vec 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd7485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import font_manager as fm\n",
    "from matplotlib import rc\n",
    "\n",
    "model = Word2Vec(contents, window = 2, min_count = 10)\n",
    "model\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "model.wv.save_word2vec_format('c:\\\\naver_qna\\\\w2v_contents_model')   #모델 저장\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eca7eb",
   "metadata": {},
   "source": [
    "# 4. 단어 유사성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(model.wv.most_similar('우울증'), columns = [['우울증','우울증'],['word', 'similarity']])\n",
    "df2 = pd.DataFrame(model.wv.most_similar('생각'), columns = [['생각','생각'],['word', 'similarity']])\n",
    "df3 = pd.DataFrame(model.wv.most_similar('사람'), columns = [['사람','사람'],['word', 'similarity']])\n",
    "df4 = pd.DataFrame(model.wv.most_similar('엄마'), columns = [['엄마','엄마'],['word', 'similarity']])\n",
    "\n",
    "sim_df = pd.DataFrame(pd.concat([df1, df2, df3, df4], axis = 1))\n",
    "sim_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f6676",
   "metadata": {},
   "source": [
    "# 5. 차원축소 및 군집분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components = 2)\n",
    "tsne\n",
    "\n",
    "vocab = model.wv.vocab\n",
    "similarity = model[vocab]\n",
    "similarity #(11420, 100)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "transform_similarity = tsne.fit_transform(similarity) # 2차원으로 변환됨\n",
    "con_df = pd.DataFrame(transform_similarity, index = vocab, columns = ['x','y'])\n",
    "con_df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f975a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "predict = kmeans.fit_predict(con_df)\n",
    "predict\n",
    "\n",
    "km_result = con_df\n",
    "km_result['predict'] = predict\n",
    "\n",
    "import seaborn as sns\n",
    "plt.rc('font',family = 'Malgun Gothic')  # 한글 안깨지게 하는 코드\n",
    "plt.rcParams['axes.unicode_minus'] = False # 특수문자 안깨지는 코드\n",
    "\n",
    "sns.lmplot('x','y', data = km_result, fit_reg = False, size = 6, hue = 'predict', legend = False)\n",
    "# plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c = 'red', marker = '*', s = 50, label = '클러스터 중심')\n",
    "\n",
    "plt.legend(['군집1','군집2','군집3','군집4'])\n",
    "plt.show()\n",
    "\n",
    "##################################################\n",
    "\n",
    "# 클러스터 중심점 찍기\n",
    "import seaborn as sns\n",
    "plt.rc('font',family = 'Malgun Gothic')  # 한글 안깨지게 하는 코드\n",
    "\n",
    "sns.lmplot('x','y', data = km_result, fit_reg = False, size = 6, hue = 'predict', legend = False,\n",
    "          palette = ['lightsteelblue', 'bisque','darkseagreen','lightpink'])\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c = 'red', marker = '*', s = 50, label = '클러스터 중심')\n",
    "\n",
    "plt.legend(['군집1','군집2','군집3','군집4','클러스터 중심'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1fdcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 군집1\n",
    "import seaborn as sns\n",
    "plt.rc('font',family = 'Malgun Gothic')  # 한글 안깨지게 하는 코드\n",
    "\n",
    "sns.lmplot('x','y', data = km_result, fit_reg = False, size = 6, hue = 'predict', legend = False,\n",
    "          palette = ['lightsteelblue', 'lightgray','lightgray','lightgray'])\n",
    "plt.scatter(kmeans.cluster_centers_[0,0], kmeans.cluster_centers_[0,1], c = 'red', marker = '*', s = 50, label = '클러스터 중심')\n",
    "\n",
    "plt.title('[군집 1]\\n', size = 20)\n",
    "plt.show()\n",
    "\n",
    "# 군집2\n",
    "import seaborn as sns\n",
    "plt.rc('font',family = 'Malgun Gothic')  # 한글 안깨지게 하는 코드\n",
    "\n",
    "sns.lmplot('x','y', data = km_result, fit_reg = False, size = 6, hue = 'predict', legend = False,\n",
    "          palette = ['lightgray', 'bisque','lightgray','lightgray'])\n",
    "plt.scatter(kmeans.cluster_centers_[1,0], kmeans.cluster_centers_[1,1], c = 'red', marker = '*', s = 50, label = '클러스터 중심')\n",
    "\n",
    "plt.title('[군집 2]\\n', size = 20)\n",
    "plt.show()\n",
    "\n",
    "# 군집3\n",
    "import seaborn as sns\n",
    "plt.rc('font',family = 'Malgun Gothic')  # 한글 안깨지게 하는 코드\n",
    "\n",
    "sns.lmplot('x','y', data = km_result, fit_reg = False, size = 6, hue = 'predict', legend = False,\n",
    "          palette = ['lightgray', 'lightgray','darkseagreen','lightgray'])\n",
    "plt.scatter(kmeans.cluster_centers_[2,0], kmeans.cluster_centers_[2,1], c = 'red', marker = '*', s = 50, label = '클러스터 중심')\n",
    "\n",
    "plt.title('[군집 3]\\n', size = 20)\n",
    "plt.show()\n",
    "\n",
    "# 군집4\n",
    "import seaborn as sns\n",
    "plt.rc('font',family = 'Malgun Gothic')  # 한글 안깨지게 하는 코드\n",
    "\n",
    "sns.lmplot('x','y', data = km_result, fit_reg = False, size = 6, hue = 'predict', legend = False,\n",
    "          palette = ['lightgray', 'lightgray','lightgray','lightpink'])\n",
    "plt.scatter(kmeans.cluster_centers_[3,0], kmeans.cluster_centers_[3,1], c = 'red', marker = '*', s = 50, label = '클러스터 중심')\n",
    "\n",
    "plt.title('[군집 4]\\n', size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90255bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "cluster1 = km_df[(km_df.predict == 0 )&(km_df.word.apply(lambda x: len(x) > 1))]\n",
    "\n",
    "dist_cluster1 = []\n",
    "for i,k in zip(cluster1['x'], cluster1['y']): \n",
    "    dist_cluster1.append(math.sqrt(abs(i - kmeans.cluster_centers_[0,0])**2 + abs(k - kmeans.cluster_centers_[0,1])**2))\n",
    "\n",
    "cluster1['dist_center'] = dist_cluster1\n",
    "cluster1\n",
    "\n",
    "cluster1 = cluster1.sort_values(by = 'dist_center', ascending = True).head(10)\n",
    "\n",
    "##############################################\n",
    "\n",
    "cluster2 = km_df[(km_df.predict == 1 )&(km_df.word.apply(lambda x: len(x) > 1))]\n",
    "\n",
    "dist_cluster2 = []\n",
    "for i,k in zip(cluster2['x'], cluster2['y']): \n",
    "    dist_cluster2.append(math.sqrt(abs(i - kmeans.cluster_centers_[1,0])**2 + abs(k - kmeans.cluster_centers_[1,1])**2))\n",
    "\n",
    "cluster2['dist_center'] = dist_cluster2\n",
    "cluster2\n",
    "\n",
    "cluster2 = cluster2.sort_values(by = 'dist_center', ascending = True).head(10)\n",
    "\n",
    "##############################################\n",
    "\n",
    "cluster3 = km_df[(km_df.predict == 2 )&(km_df.word.apply(lambda x: len(x) > 1))]\n",
    "\n",
    "dist_cluster3 = []\n",
    "for i,k in zip(cluster3['x'], cluster3['y']): \n",
    "    dist_cluster3.append(math.sqrt(abs(i - kmeans.cluster_centers_[2,0])**2 + abs(k - kmeans.cluster_centers_[2,1])**2))\n",
    "\n",
    "cluster3['dist_center'] = dist_cluster3\n",
    "cluster3\n",
    "\n",
    "cluster3 = cluster3.sort_values(by = 'dist_center', ascending = True).head(10)\n",
    "\n",
    "##############################################\n",
    "\n",
    "cluster4 = km_df[(km_df.predict == 3 )&(km_df.word.apply(lambda x: len(x) > 1))]\n",
    "\n",
    "dist_cluster4 = []\n",
    "for i,k in zip(cluster4['x'], cluster4['y']): \n",
    "    dist_cluster4.append(math.sqrt(abs(i - kmeans.cluster_centers_[3,0])**2 + abs(k - kmeans.cluster_centers_[3,1])**2))\n",
    "\n",
    "cluster4['dist_center'] = dist_cluster4\n",
    "cluster4\n",
    "\n",
    "cluster4 = cluster4.sort_values(by = 'dist_center', ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3944fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(pd.concat([cluster1,\n",
    "                            cluster2,\n",
    "                            cluster3,\n",
    "                            cluster4], axis = 1))\n",
    "a.columns =  [['군집1','군집1','군집2','군집2','군집3','군집3','군집4','군집4'],\n",
    "           ['word','cnt','word','cnt','word','cnt','word','cnt']]\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
